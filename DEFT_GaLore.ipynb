{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/SurgicalAICopilot/blob/main/DEFT_GaLore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "md_Fo8mzN-so"
      },
      "outputs": [],
      "source": [
        "# Surgical LLM Agent Demo\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create output directories in Google Drive\n",
        "import os\n",
        "output_base_dir = '/content/drive/MyDrive/surgical_llm_demo'\n",
        "model_dir = f'{output_base_dir}/models/DEFT-GaLore_weight'\n",
        "data_dir = f'{output_base_dir}/datasets'\n",
        "results_dir = f'{output_base_dir}/results'\n",
        "metrics_dir = f'{output_base_dir}/metrics'\n",
        "\n",
        "# Create directories if they don't exist\n",
        "for directory in [output_base_dir, model_dir, data_dir, results_dir, metrics_dir]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "print(f\"Created output directories in Google Drive at: {output_base_dir}\")\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q transformers==4.45.1 datasets==3.0.1 evaluate==0.4.3 gdown nltk torch==2.4.0 torchvision==0.19.0\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q bitsandbytes>=0.43.2\n",
        "!pip install -q accelerate\n",
        "!pip install rouge_score\n",
        "\n",
        "# Download necessary NLTK data\n",
        "import nltk\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Download model weights using gdown (NOTE: Replace with your actual file ID)\n",
        "print(\"Downloading model weights...\")\n",
        "MODEL_FILE_ID = \"1l5tJ41cQa0M8z0UF8Az96DX1sZMAV0Yt\"  # Replace with the actual file ID from Google Drive\n",
        "#https://drive.google.com/drive/folders/1l5tJ41cQa0M8z0UF8Az96DX1sZMAV0Yt?usp=sharing\n",
        "\n",
        "# Check if model is already downloaded\n",
        "if not os.path.exists(f\"{model_dir}/config.json\"):\n",
        "    !gdown --folder {MODEL_FILE_ID} -O {model_dir}\n",
        "    print(f\"Model weights downloaded to {model_dir}\")\n",
        "else:\n",
        "    print(f\"Model weights already exist at {model_dir}\")\n",
        "\n",
        "# Download datasets\n",
        "print(\"Downloading datasets...\")\n",
        "DATASET_FILE_IDS = {\n",
        "    \"Surgical-VQA_V.csv\": \"1rjv3PzKHqz5BjJn8anR2jSwNTPS2k0Ak\",\n",
        "    \"Overlaying_V.csv\": \"1gFpt8kjoc0kzTBXXiRlYgwDC-HzlSGAr\",\n",
        "    \"Segment-MRI_V.csv\": \"1rSJfPEqg24fhk4MybqpRw652orLfgojc\",\n",
        "    \"Segment-Video_V.csv\": \"1lo0xEKcJgMPy0T0AXfRIjNbrSdJyrXkR\",\n",
        "    \"Detect-Instrument_V.csv\": \"1A4c5ieW6P_oqMnWRybl6NmtmTshsPX1n\",\n",
        "    \"2model_V.csv\": \"1dl_81gH1o06ZYLn1FuL8J4INxq3cLqnu\",\n",
        "    \"3model_V.csv\": \"1WwLigg0kjRHyxkOaSYc8V2M8GFK9qlc9\"\n",
        "}\n",
        "\n",
        "# https://drive.google.com/file/d/1rjv3PzKHqz5BjJn8anR2jSwNTPS2k0Ak/view?usp=drive_link\n",
        "# https://drive.google.com/file/d/1gFpt8kjoc0kzTBXXiRlYgwDC-HzlSGAr/view?usp=drive_link\n",
        "# https://drive.google.com/file/d/1rSJfPEqg24fhk4MybqpRw652orLfgojc/view?usp=drive_link\n",
        "# https://drive.google.com/file/d/1lo0xEKcJgMPy0T0AXfRIjNbrSdJyrXkR/view?usp=drive_link\n",
        "# https://drive.google.com/file/d/1A4c5ieW6P_oqMnWRybl6NmtmTshsPX1n/view?usp=drive_link\n",
        "# https://drive.google.com/file/d/1dl_81gH1o06ZYLn1FuL8J4INxq3cLqnu/view?usp=drive_link\n",
        "# https://drive.google.com/file/d/1WwLigg0kjRHyxkOaSYc8V2M8GFK9qlc9/view?usp=drive_link\n",
        "\n",
        "# Download each dataset\n",
        "for filename, file_id in DATASET_FILE_IDS.items():\n",
        "    if not os.path.exists(f\"{data_dir}/{filename}\"):\n",
        "        !gdown {file_id} -O {data_dir}/{filename}\n",
        "        print(f\"Downloaded {filename}\")\n",
        "    else:\n",
        "        print(f\"{filename} already exists\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Fuctions"
      ],
      "metadata": {
        "id": "ii0pdy3jOJ71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import pandas as pd\n",
        "import re\n",
        "import evaluate\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# ---------------------------- Utility Function ----------------------------\n",
        "def generate_SM(que: str) -> str:\n",
        "    return (\n",
        "        \"You are a surgical AI agent assisting in pituitary surgery. Your job is to handle surgeons' queries efficiently by choosing appropriate text-promptable AI models and generating corresponding prompts.\\n\"\n",
        "        \"Available models: Segment-Video, Segment-MRI, Track-Instrument, Surgical-VQA, Overlaying.\\n\"\n",
        "        \"Question: {que}\\n\"\n",
        "        \"- Use ONE model if query focuses on a single, simple aspect:\\n\"\n",
        "        \"Example (single-model):\\n\"\n",
        "        \"Model: Segment-Video\\nPrompt: Segment the sella in the video.\\n\"\n",
        "        \"- Use MULTIPLE models if query requires several types of information:\\n\"\n",
        "        \"Example (multi-model):\\n\"\n",
        "        \"Step1:\\nModel: Segment-MRI\\nPrompt: Segment the pituitary tumor from MRI.\\n\"\n",
        "        \"Step2:\\nModel: Segment-Video\\nPrompt: Segment the sella in the video.\\n\"\n",
        "        \"Now, follow the same format to answer the provided questionâ€”no extra text, labels, or formatting.\"\n",
        "    ).format(que=que)\n",
        "\n",
        "def generate_answer(question, model, tokenizer):\n",
        "    model.eval()\n",
        "    question = generate_SM(question)\n",
        "    input_text = f\"Query:\\n{question}\\nResponse:\\n\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)\n",
        "    answer = tokenizer.decode(output[0], skip_special_tokens=True).split(\"Response:\\n\")[-1].strip()\n",
        "    return answer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2SiZuvOsOLxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load model"
      ],
      "metadata": {
        "id": "4cpn5t7iOQHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the model using BitsAndBytes for efficient loading\n",
        "print(\"Loading model...\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "try:\n",
        "    # Load model and tokenizer\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_dir,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "_-xgtBZPORzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define Input question\n",
        "# Single model case:\n",
        "print(\"The question of single model case is:\\nCan you show me where to be careful and not make incisions?\\n\")\n",
        "# Define the sample query question here:\n",
        "question1 = \"Can you show me where to be careful and not make incisions?\"\n",
        "\n",
        "answer1 = generate_answer(question1, model, tokenizer)\n",
        "print(\"The answer generated by agent is:\")\n",
        "print(answer1)\n",
        "\n",
        "# Multiple model case:\n",
        "print(\"\\nThe question of multiple models case is:\\nIdentift if I'm prepared to transition to tumor excision with the pituitary rongeur?\\n\")\n",
        "question2 = \"Identift if I'm prepared to transition to tumor excision with the pituitary rongeur?\"\n",
        "\n",
        "answer2 = generate_answer(question2, model, tokenizer)\n",
        "print(\"The answer generated by agent is:\")\n",
        "print(answer2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkmjUPd2EFeY",
        "outputId": "947b71ba-be07-43bb-93d1-92b62f6e78a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The question of single model case is:\n",
            "Can you show me where to be careful and not make incisions?\n",
            "\n",
            "The answer generated by agent is:\n",
            "Model: Segment-Video\n",
            "Prompt: Segment the internal skull structures.\n",
            "\n",
            "The question of multiple models case is:\n",
            "Identift if I'm prepared to transition to tumor excision with the pituitary rongeur?\n",
            "\n",
            "The answer generated by agent is:\n",
            "Step1:\n",
            "Model: Track-Instrument\n",
            "Prompt: Verify the presence and correct usage of the pituitary rongeur.\n",
            "Step2:\n",
            "Model: Surgical-VQA\n",
            "Prompt: Confirm readiness for the tumour excision step and provide brief recommendations.\n"
          ]
        }
      ]
    }
  ]
}